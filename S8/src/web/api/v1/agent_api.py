from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import asyncio
from typing import List, Dict, Any, Optional
import logging
import re
import json
from pathlib import Path



from pathlib import Path
import sys
ROOT = Path(__file__).resolve().parents[4]  # This gets /Users/ravi/EAG-TheShadowCloneAI/S8

if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# Import your existing agent functionality
from src.core.agent.agent import main as agent_main

# Import memory components to access stored plans
from src.core.agent.common.memory_store.memory import MemoryManager, MemoryItem
from src.common.logger.logger import get_logger

logger = get_logger()

app = FastAPI(title="Agent-Based Search API")

# Define models similar to search_api.py for consistency
class AgentQuery(BaseModel):
    query: str
    session_id: str = "user_1"


class AgentResponse(BaseModel):
    results: str

# Function to extract URL from doc_id if it appears to be a URL (same as in search_api.py)
def extract_url_from_doc_id(doc_id: str) -> Optional[str]:
    """Extract URL from doc_id if it appears to be a URL"""
    url_pattern = re.compile(r'^https?://\S+$')
    if url_pattern.match(doc_id):
        return doc_id
    return None

# Utility function to extract structured data from agent responses
def extract_structured_data(text: str) -> Dict[str, Any]:
    """
    Extract structured data like URLs, titles, and source types from text.
    
    Args:
        text: The text to extract data from
        
    Returns:
        Dictionary containing extracted structured data
    """
    data = {
        "urls": [],
        "source_types": [],
        "titles": []
    }
    
    # Extract URLs
    url_pattern = re.compile(r'https?://\S+')
    urls = url_pattern.findall(text)
    for url in urls:
        # Clean URL by removing trailing punctuation
        if url[-1] in ['.', ',', ')', ']', ';', ':']:
            url = url[:-1]
        data["urls"].append(url)
    
    # Extract source types
    source_type_patterns = [
        (r'html\b', "html"),
        (r'website\b', "html"),
        (r'web\b', "html"),
        (r'pdf\b', "pdf"),
        (r'document\b', "document"),
        (r'txt\b', "text")
    ]
    
    for pattern, source_type in source_type_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            data["source_types"].append(source_type)
    
    # Extract titles
    title_patterns = [
        r'title[:\s]+["\'](.*?)["\']',
        r'titled\s+["\'](.*?)["\']',
        r'called\s+["\'](.*?)["\']'
    ]
    
    for pattern in title_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            data["titles"].append(match.group(1))
    
    return data

def extract_results_from_final_answer(final_answer: str) -> List[Dict[str, Any]]:
    """
    Parse the FINAL_ANSWER response from the agent and convert it to SearchResult objects.
    Enhanced to robustly extract URLs and source types from the agent's response.
    """
    # Check for None value and provide a default
    if final_answer is None:
        logger.warning("Agent returned None response, using default message")
        answer_text = "No response was generated by the agent."
    # Strip the "FINAL_ANSWER:" prefix if present
    elif final_answer.startswith("FINAL_ANSWER:"):
        answer_text = final_answer[len("FINAL_ANSWER:"):].strip()
    elif final_answer.startswith("ðŸ’¡ Final Answer:"):
        answer_text = final_answer[len("ðŸ’¡ Final Answer:"):].strip()
    else:
        answer_text = final_answer.strip()
    
    # Advanced metadata extraction from the response
    source_type = "agent"
    doc_id = "agent-response"
    title = "Agent Response"
    url = None
    
    # Extract URL patterns from the text
    url_pattern = re.compile(r'https?://\S+')
    url_matches = url_pattern.findall(answer_text)
    
    # Extract any source information
    source_patterns = [
        # Match source patterns like (Source: example.com)
        r'\(Source:?\s*(https?://[^)]+)\)',
        # Match from patterns like "from website.com"
        r'from\s+(https?://\S+)',
        # Match general reference to website
        r'website[:\s]+(https?://\S+)',
        # Match references using quotes
        r'["\'](https?://\S+)["\']',
        # Match "source:" or "reference:" followed by URL
        r'(?:source|reference)[:\s]+(https?://\S+)',
        # Match URLs following "See more at:" pattern
        r'(?:see more at|learn more at|view at)[:\s]+(https?://\S+)'
    ]
    
    found_url = None
    
    # Look for explicit source references first
    for pattern in source_patterns:
        matches = re.search(pattern, answer_text, re.IGNORECASE)
        if matches:
            found_url = matches.group(1)
            # Clean up URL by removing trailing punctuation
            if found_url[-1] in ['.', ',', ')', ']', ';', ':', '"', "'"]:
                found_url = found_url[:-1]
            break
    
    # If no explicit source reference, use any URL found
    if not found_url and url_matches:
        found_url = url_matches[0]
        # Clean up URL by removing trailing punctuation
        if found_url[-1] in ['.', ',', ')', ']', ';', ':', '"', "'"]:
            found_url = found_url[:-1]
    
    # Determine source type based on the content
    if found_url:
        url = found_url
        # Set URL as doc_id if it's a website
        doc_id = found_url
        
        # Determine source type based on URL or content
        if ".html" in found_url.lower() or "html" in answer_text.lower() or "web" in answer_text.lower():
            source_type = "html"
        elif ".pdf" in found_url.lower() or "pdf" in answer_text.lower():
            source_type = "pdf"
        else:
            # Default to html for web URLs
            source_type = "html"
        
        # Try to extract title from content
        title_match = re.search(r'title[:\s]+["\'](.*?)["\']', answer_text, re.IGNORECASE)
        if title_match:
            title = title_match.group(1)
        elif "html" in source_type:
            # Extract domain name as title
            domain_match = re.search(r'https?://(?:www\.)?([^/]+)', found_url)
            if domain_match:
                title = f"Content from {domain_match.group(1)}"
    
    # Create the result dictionary
    result = {
        "chunk": answer_text
    }
    
    # Add URL if found
    if url:
        result["url"] = url
    
    return [result]

def retrieve_last_plan_from_memory(session_id: str) -> Optional[str]:
    """
    Retrieve the last plan stored in memory with the "plan" tag for a given session.
    
    Args:
        session_id: The ID of the session to retrieve the plan for
        
    Returns:
        The text of the last plan, or None if no plan was found
    """
    try:
        # Initialize memory manager
        memory = MemoryManager()
        
        # Retrieve all memories with "plan" tag for the session
        plans = memory.retrieve(
            query="plan",  # Using "plan" as a simple query to match plan items
            top_k=10,  # Get multiple plans to find the last one
            tag_filter=["plan"],  # Filter by "plan" tag
            session_filter=session_id  # Filter by session ID
        )
        # Sort plans by timestamp to get the latest one (descending order)
        if plans:
            # Sort plans by timestamp (newest first)
            sorted_plans = sorted(plans, key=lambda x: x.timestamp if x.timestamp else "", reverse=True)
            
            # Return the text of the most recent plan
            logger.info(f"Retrieved latest plan from memory: {sorted_plans[0].text[:50]}...")
            return sorted_plans[0].text
        
        logger.warning(f"No plans found in memory for session {session_id}")
        return None
    
    except Exception as e:
        logger.error(f"Error retrieving plan from memory: {e}", exc_info=True)
        return None

def retrieve_vector_search_results(session_id: str) -> Optional[Dict[str, Any]]:
    """
    Retrieve any vector search results stored in memory for a given session.
    
    Args:
        session_id: The ID of the session to retrieve results for
        
    Returns:
        Dictionary containing the tool output and extracted URLs, or None if no results found
    """
    try:
        # Initialize memory manager
        memory = MemoryManager()
        
        # Retrieve all memories with "tool_output" type and "vector_search" tool name for the session
        search_results = memory.retrieve(
            query="vector_search",  # Using the tool name as a query
            top_k=5,  # Get multiple results in case there were multiple searches
            type_filter="tool_output",  # Filter by type
            session_filter=session_id  # Filter by session ID
        )
        # Also try with tag filter for extra insurance
        tag_results = memory.retrieve(
            query="vector_search",
            top_k=5,
            tag_filter=["tool_result", "vector_search"],
            session_filter=session_id
        )

        all_results = list(search_results)
        for result in tag_results:
            if result not in all_results:
                all_results.append(result)

        if not all_results:
            logger.warning(f"No vector search results found in memory for session {session_id}")
            return None
        
        # Sort by timestamp (newest first)
        sorted_results = sorted(all_results, key=lambda x: x.timestamp if x.timestamp else "", reverse=True)
        
        # Process the results to extract URLs
        results_data = {}
        urls = []
        source_types = []
        titles = []
        
        for result in sorted_results:
            # Try to parse the result text as JSON (it might be a serialized SearchResponse)
            try:
                # Check if the text is valid JSON
                if '{' in result.text and '}' in result.text:
                    # Extract the JSON part if text contains other content
                    json_str = result.text
                    # If there's text before the JSON, extract just the JSON part
                    if not json_str.strip().startswith('{'):
                        json_start = json_str.find('{')
                        if json_start >= 0:
                            json_str = json_str[json_start:]
                    
                    # Try to parse the JSON
                    data = json.loads(json_str)
                    
                    # Extract any URLs and titles from the results
                    if "results" in data:
                        for item in data["results"]:
                            if "url" in item and item["url"]:
                                urls.append(item["url"])
                            if "title" in item:
                                titles.append(item["title"])
                            if "source_type" in item:
                                source_types.append(item["source_type"])
            except (json.JSONDecodeError, ValueError):
                # If JSON parsing fails, try to extract URLs directly from the text
                url_pattern = re.compile(r'https?://\S+')
                found_urls = url_pattern.findall(result.text)
                for url in found_urls:
                    # Clean URL by removing trailing punctuation
                    if url[-1] in ['.', ',', ')', ']', ';', ':', '"', "'"]:
                        url = url[:-1]
                    urls.append(url)
        
        # Store the extracted data
        results_data["urls"] = urls
        results_data["source_types"] = source_types
        results_data["titles"] = titles
        results_data["raw_results"] = [r.text for r in sorted_results]
        
        logger.info(f"Retrieved {len(urls)} URLs from vector search results in memory")
        return results_data
    
    except Exception as e:
        logger.error(f"Error retrieving vector search results from memory: {e}", exc_info=True)
        return None

@app.post("/agent", response_model=AgentResponse)
async def search(query_params: AgentQuery):
    """
    Agent-based search endpoint that processes queries and returns results
    
    - **query**: The search query text
    - **session_id**: Session identifier (default: "user_1")
    """
    try:
        logger.info(f"Received search query: {query_params.query}")
        
        # Enhance the query to request source URLs if appropriate
        enhanced_query = query_params.query
        
        # If query seems like a search for information, explicitly request URLs
        search_terms = ["find", "search", "look up", "research", "information about", "details on", "website", "link"]
        if any(term in query_params.query.lower() for term in search_terms):
            # Modify the query to explicitly ask for URLs if relevant
            if not any(term in enhanced_query.lower() for term in ["include url", "provide url", "with source"]):
                enhanced_query += " (Please include source URLs in your response if available)"
        
        logger.info(f"Enhanced query: {enhanced_query}")
        session_id = query_params.session_id
        
        # Call your agent's main function with the enhanced query
        agent_response = await agent_main(enhanced_query, session_id)
        
        logger.info(f"Agent response: {agent_response}")
 
        # Process the response
        if agent_response is None:
            # Fall back to checking memory for the last plan
            memory = MemoryManager()
            plans = memory.retrieve(
                query="plan",
                top_k=10,
                tag_filter=["plan"],
                session_filter=session_id
            )
            
            if plans:
                # Use the most recent plan
                sorted_plans = sorted(plans, key=lambda x: x.timestamp if x.timestamp else "", reverse=True)
                plan_content = sorted_plans[0].text
                
                # Format as a response if needed
                if "Plan:" in plan_content:
                    plan_content = plan_content.split("Plan:", 1)[1].strip()
                
                if not plan_content.startswith("FINAL_ANSWER:"):
                    plan_content = f"FINAL_ANSWER: Based on my analysis plan: {plan_content}"
                
                agent_response = plan_content
            else:
                agent_response = "FINAL_ANSWER: The agent processed your query but didn't return a proper response. Please try again or contact support."
        
        # Format the response as a string for the expected return type
        final_result = agent_response
        if final_result.startswith("FINAL_ANSWER:"):
            final_result = final_result[len("FINAL_ANSWER:"):].strip()
            
        return AgentResponse(results=final_result)
    
    except Exception as e:
        logger.error(f"Search error: {e}", exc_info=True)
        return AgentResponse(results=f"An error occurred while processing your query: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)